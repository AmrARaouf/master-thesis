\chapter{Analysis}\label{ch:analysis}

The analysis process results in a model of the system that aims to be correct, complete, consistent and unambiguous \cite{bruegge2004object}. In this chapter, we focus on scenario-based analysis. Section \ref{requirements}, identifies the functional and nonfunctional requirements of the system. In section \ref{system_models}, we describe a use case model that depicts the different interactions between the users and the system. Furthermore, we identify and study the objects of our system in the Analysis Object Model. Finally, 3 dynamic models that depict workflows in our system are presented.


\section{Requirements}\label{requirements}

A requirement is a feature that the system must have or a constraint that it must satisfy to be accepted by the client \cite{bruegge2004object}. Requirements elicitation requires domain knowledge of the problem statement as well as experience in building software systems.

We design a system that uses synthetic and real images to classify different small parts. To do so, we create 3D models of our small parts. We subsequently generate a real image set for each small part, and a synthetic image set from the small part's 3D model. The real and synthetic image sets are combined to form a dataset, which we use to train a classification model based on convolutional neural networks. In this section, the concrete functional and nonfunctional requirements needed to build our system are defined.

\subsection{Functional Requirements}
Functional requirements (FRs) describe the interactions between the system and the its environment independent of its implementation \cite{bruegge2004object}.

The system's main function is to classify images of different small parts. Furthermore, the system needs to be dynamic and scalable to accommodate new small parts. To ensure that the system is able to classify images accurately, while leveraging synthetic input images, we introduce the following Functional Requirements.

\begin{itemize}
  \item [FR1] \textbf{Create 3D Model}: The system should use a 3D scanner to create a 3D model of a given small part.

  \item [FR2] \textbf{Generate Synthetic Image Set}: Given a 3D model of a small part, the system should place the 3D model in a synthetic scene and subsequently render a set of 2D synthetic images of the scene. The synthetic images should capture the 3D model in different positions.

  \item [FR3] \textbf{Set 3D Model Transformation Range}: To capture the model from different positions the model should be rotated and translated in the synthetic scene. The system should allow the user to define a rotation and translation range for each 3D model to avoid the risk of placing the 3D model in unrealistic positions.

  \item [FR4] \textbf{Generate Real Image Set}: Given a small part, the system should generate a set of real images of the small part.

  \item [FR5] \textbf{Split Input Dataset}: The synthetic and real image sets are combined to form a dataset. The system should split the dataset into a training set, a testing set and a validation set to train and evaluate a classification model. Moreover, the system should allow the user to set the ratio of synthetic to real images in the training set.

  \item [FR6] \textbf{Train Classification Model}: The system should have the capacity to train a classification model given a training and a validation set.

  \item [FR7] \textbf{Evaluate Classification Model}: The system should provide a mechanism to evaluate the performance of a given classification model.
\end{itemize}

\subsection{Nonfunctional Requirements}
Nonfunctional Requirements (NFRs) are key system requirements that apply to the system as a whole \cite{bruegge2004object}. To maintain the system's general requirements we define the following nonfunctional requirements.

\begin{itemize}
  \item [NFR1] \textbf{Performance}: A performance requirement is the measure of a quantifiable attribute of our system. In our case we would like to track our system's \textbf{classification accuracy}.
  \\We first define our system's accuracy for a single class to be the percentage of the given class's testing images that the classification model correctly predicts. We consequently define our system's classification accuracy to be the classification model's average prediction accuracy over each class.

  \item [NFR2] \textbf{Adaptability}: Adaptability is the ability to change a system to deal with additional application domain concepts \cite{bruegge2004object}. Our system should accommodate for the addition of new small parts to the set of target classes that our classification model is trained to identify. This addition can be done even after the system has been deployed.
\end{itemize}


\section{System Models}\label{system_models}

In this section we take a closer look at the system models. In section \ref{use_case_model}, we establish the main actors of our system and determine the actions that they can undertake. Section \ref{analysis_object_model}, identifies the objects of our system, examines their behavior and studies the relationship between them. Lastly, in section \ref{dynamic_model} we present activity diagrams that describe the workflows of our system.

\subsection{Use Case Model}\label{use_case_model}
Use case models represent the relationship between the user group of a system and the general functions that they can execute. A use case model can reduce the complexity of a system and increase its understandability.

Figure \ref{fig:UCM} depicts the use case model of our system. There are three main protagonists in our system. The \textit{3DModelCreator} is responsible for creating 3D models for the given small parts. The \textit{DataGenerator} uses the small parts and their corresponding 3D models to generate synthetic and real image sets. Finally, the \textit{MachineLearningEngineer} is responsible for creating a training, a validation and a testing sets. Moreover, the \textit{MachineLearningEngineer} trains the classification model to identify a set of small parts, and evaluates the model's performance.

The use case model can be divided into 4 main functions; namely creating 3D models, generating synthetic images, generating real images, and training/evaluating a classification model.

\begin{figure}[h]
\centering
  \includegraphics[width=\textwidth]{UCM}
\caption{The use case model displays the DataGenerator and the MachineLearningEngineer and the actions that they can undertake.}
\label{fig:UCM}
\end{figure}

\clearpage
\subsubsection{Creating 3D Models}
In order to create synthetic images, we need to create 3D models of our small parts. To do so, the 3DModelCreator uses a 3D scanner and compatible 3D scanning software to create the model.

\begin{usecase}
  \addtitle{Use Case}{Create3DModel}

  \addfield{Participating Actors}{3DModelCreator}

  \addscenario{Flow of Events}{
    \item The 3DModelCreator places the small part on a flat surface.
    \item The 3DModelCreator uses the 3D scanner to scan images of the small part from all directions.
    \item The 3D scanning software combines the scanned images to create a 3D model.
  }

  \additemizedfield{Entry Condition}{
    \item A small part has been chosen for scanning.
    \item The compatible 3D scanning software is open.
    \item The 3D scanner is turned on and recognized by the 3D scanning software.
  }

  \additemizedfield{Exit Condition}{
    \item A 3D model of the chosen small part has been created.
  }

  \additemizedfield{Quality Requirements}{
    \item The 3D model is as realistic as possible.
  }
\end{usecase}

\newpage
\subsubsection{Generating Synthetic Images}
The created 3D models are used to generate a synthetic image set. To increase the understandability of our model, we separate the use case that concerns modeling the 3D model as \textit{SetTransformationRange}, and the use case that deals with the synthetic scene environment as \textit{SetUpSyntheticScene}. Both are included in the use case \textit{GenerateSyntheticImageSet}.

\begin{usecase}
  \addtitle{Use Case}{SetTransformationRange}

  \addfield{Participating Actors}{DataGenerator}

  \addscenario{Flow of Events}{
    \item In the modeling software, the DataGenerator chooses the transformations that are applicable to the small part's 3D model.
    \item For each transformation, the DataGenerator sets the range of each transformation attribute that will be later used to generate a random position for the synthetic image of the small part.
  }

  \additemizedfield{Entry Condition}{
    \item A 3D model of a small part has been created.
  }

  \additemizedfield{Exit Condition}{
    \item All the transformation ranges for the 3D model have been set.
  }

  \additemizedfield{Quality Requirements}{
    \item The 3D transformation ranges should reflect the real-life transformations that the small parts can undergo.
    \item The 3D model should remain within camera viewfield.
  }
\end{usecase}

\newpage
\begin{usecase}
  \addtitle{Use Case}{SetUpSyntheticScene}

  \addfield{Participating Actors}{DataGenerator}

  \addscenario{Flow of Events}{
    \item In the modeling software, the DataGenerator places the 3D model of the small part on a horizontal plane as if lying on a table.
    \item The DataGenerator sets the background of the horizontal plane to mimic the background upon which the small part's real images are taken.
    \item The DataGenerator sets the lighting condition of the scene to reflect the lighting condition of the environment where the small part's real images are taken.
  }

  \additemizedfield{Entry Condition}{
    \item A 3D model of a small part has been created.
  }

  \additemizedfield{Exit Condition}{
    \item In the modeling software, the 3D model of the small part should be lying on a horizontal plane.
    \item The background and lighting condition should reflect the environment of the real image set.
  }

  \additemizedfield{Quality Requirements}{
    \item The synthetic scene should look as photorealistic as possible. This helps the classification model achieve a higher classification accuracy using the synthetic image set.
  }
\end{usecase}

\newpage
\begin{usecase}
  \addtitle{Use Case}{GenerateSyntheticImageSet}

  \addfield{Participating Actors}{DataGenerator}

  \addscenario{Flow of Events}{
    \item The DataGenerator specifies the desired number of output images in the synthetic image set.
    \item The modeling software generates the desired number of output synthetic images. Each image is a rendition of the synthetic scene after the transformations have been applied to the 3D model.
  }

  \additemizedfield{Entry Condition}{
    \item DataGenerator has set the transformation ranges of the 3D model.
    \item DataGenerator has set up the synthetic scene.
  }

  \additemizedfield{Exit Condition}{
    \item The DataGenerator obtains a set of synthetic images for the specified small part. The image set can be later used to train a classification model.
  }

  \additemizedfield{Quality Requirements}{
    \item The generated image dimensions should match the input dimensions required by the classification model.
  }
\end{usecase}

\newpage
\subsubsection{Generating Real Images}
The DataGenerator should strive to create an environment that is easy to model in 3D software. This will enable the creation of photo-realistic synthetic images.

\begin{usecase}
  \addtitle{Use Case}{GenerateRealImageSet}

  \addfield{Participating Actors}{DataGenerator}

  \addscenario{Flow of Events}{
    \item The DataGenerator places the camera over a plane.
    \item The DataGenerator sets the small part over the plane in a random position, while maintaining that the small part remains horizontal. The full small part body should remain within the viewfield of the camera.
    \item The DataGenerator takes a picture of the small part.
    \item The DataGenerator repeats steps 2 and 3 until the desired number of real images of the small part is reached.
    \item The DataGenerator resizes the images to correspond to the input image size of the classification model. 
  }

  \additemizedfield{Entry Condition}{
    \item A small part has been chosen.
  }

  \additemizedfield{Exit Condition}{
    \item The DataGenerator obtains a set of real images for the specified small part that can be later used to train the classification model.
  }

  \additemizedfield{Quality Requirements}{
    \item The lighting of the environment should eliminate shadows and light reflections on the small part surface.
  }
\end{usecase}

\newpage
\subsubsection{Training the Classification Model}
The synthetic and real image sets are combined to form a dataset. Training a classification model requires the dataset to be split into a training, a validation, and a testing set. We model this requirement as the use case \textit{SplitInputDataset}. Furthermore the actions required for setting up the classification model are modeled as the use case \textit{SetUpClassificationModel}.

\begin{usecase}
  \addtitle{Use Case}{SplitInputDataset}

  \addfield{Participating Actors}{MachineLearningEngineer}

  \addscenario{Flow of Events}{
    \item The MachineLearningEngineer chooses the number of images for the training set, validation set and testing set.
    \item The MachineLearningEngineer chooses the ratio of synthetic images to real images in the training set.
  }

  \additemizedfield{Entry Condition}{
    \item For each small part set to be classified by the classification model, the corresponding real and synthetic image sets should be generated and ready for use.
  }

  \additemizedfield{Exit Condition}{
    \item The images in the dataset are split into a training, a validation and a testing set, ready to be used by the classification model.
  }
\end{usecase}

\newpage
\begin{usecase}
  \addtitle{Use Case}{SetUpClassificationModel}

  \addfield{Participating Actors}{MachineLearningEngineer}

  \addscenario{Flow of Events}{
    \item The MachineLearningEngineer chooses a convolutional neural network architecture for the classification model.
    \item The MachineLearningEngineer sets the hyperparameters: number of batches, number of epochs, number of frozen layers in the CNN, the optimizer and the hyperparameters associated with the chosen optimizer.
  }

  \additemizedfield{Exit Condition}{
    \item The classification model architecture, optimizer and corresponding hyperparameters should be selected.
  }
\end{usecase}

\newpage
\begin{usecase}
  \addtitle{Use Case}{TrainClassificationModel}

  \addfield{Participating Actors}{MachineLearningEngineer}

  \addscenario{Flow of Events}{
    \item The MachineLearningEngineer trains the classification model on the given training and validation sets.
    \item The MachineLearningEngineer evaluates the accuracy of the classification model by studying the model's predictions on the testing set.
    \item The MachineLearningEngineer fine-tunes the hyperparameters and re-trains the system until the maximum possible accuracy is reached.
  }

  \additemizedfield{Entry Condition}{
    \item The dataset should be split.
    \item The CNN model architecture, optimizer and initial hyperparameters should be chosen.
  }

  \additemizedfield{Exit Condition}{
    \item The classification model outputs the maximum possible accuracy given the input dataset.
  }

  \additemizedfield{Quality Requirements}{
    \item The classification model's hyperparameters should be fine-tuned until the maximum possible accuracy has been reached.
  }
\end{usecase}

\newpage
\subsection{Analysis Object Model}\label{analysis_object_model}
The analysis object model is a visual dictionary of the main concepts visible to the user \cite{bruegge2004object}. The analysis object model depicts a system's entities, their corresponding attributes and functions, and illustrates the relationship between said entities. Figure \ref{fig:AOM1} and figure \ref{fig:AOM2} are represent the analysis object models of our system. We divide our model into 2 UML packages, namely, \textit{Dataset Generation} and \textit{Image Classification} depicted in figures \ref{fig:AOM1} and \ref{fig:AOM2} respectively. Dataset Generation presents the objects required to create 3d models of small parts and subsequently generate a dataset of real and synthetic images. Image Classification displays the objects used to utilize the generated dataset for the training and evaluation of a CNN model, which we use to classify our small parts.

\subsubsection{Dataset Generation}
To generate our dataset, we first need to create \textbf{3DModels} of our \textbf{SmallParts}. A \textbf{3DScanner} and a compatible \textbf{3DScanningSoftware} are used to scan a SmallPart and generate the corresponding 3DModel. Each SmallPart and its reciprocal 3DModel have a label string which it later used by the image classifier as the object class.

SmallParts and 3DModels both implement the \textbf{Transformable} interface. Each Transformable has multiple \textbf{Transformations}, and can apply the transform function. Transformation is an abstraction of \textbf{Translation} and \textbf{Rotation}. Each Transformation has an axis (x, y or z) and a range; Translations define minimum and maximum distances, while Rotations define minimum and maximum angles. Additionally, each transformation has a function to generate a random value within the defined range. This function is used to transform the object randomly before generating a real or synthetic image.

\textbf{ImageGenerator} is a generalization of \textbf{RealImageProcessor}, which uses the \textbf{Camera} to generate \textbf{RealImages}, and \textbf{3DModelingSoftware}, which generates \textbf{SyntheticImages}. ImageGenerators define the output number of images, as well as their width and height. Moreover, they apply random transformations to their target transformable before generating each image. Each 3DModelingSoftware has a \textbf{SyntheticScene} where the background and lighting conditions of the synthetic images are defined.

RealImage and SyntheticImage are both specializations of the \textbf{Image} class. Each Image has a width, height, label (corresponding to the label of the depicted SmallPart of 3DModel), and a data buffer which holds the values of the image pixels. The \textbf{Dataset} is a composition of multiple Images.

\begin{figure}[H]
\centering
  \includegraphics[width=\textwidth]{AOM1}
\caption{UML package of the analysis object model which models dataset generation.}
\label{fig:AOM1}
\end{figure}

\subsubsection{Image Classification}
After the dataset has been generated, it is split into a training set, a validation set and a testing set by the \textbf{DatasetSplitter}. The DatasetSplitter sets the size of each set as well as the ratio of synthetic to real images in the training set. The \textbf{ImageClassifier} uses the split dataset to train and evaluate a \textbf{CNNModel}. The CNNModel defines the model weights as well as the hyperparameters: batch size, number of epochs, and, number of frozen layers. CNNModel uses the \textbf{Optimizer} to adjust the model weights in the training phase.

\begin{figure}[H]
\centering
  \includegraphics[width=\textwidth]{AOM2}
\caption{UML package of the analysis object model which depicts the objects that are used for image classification.}
\label{fig:AOM2}
\end{figure}

\newpage
\subsection{Dynamic Model}\label{dynamic_model}
The dynamic model is focused on the behavior of the system. Sequence diagrams, activity diagrams and state machines are usually used to depict dynamic models \cite{bruegge2004object}. In this section we present 3 activity diagrams, each depicts a workflow in our system. Namely, the workflow to create a real image set from a small part, the workflow to create the small part's 3D model and subsequently generate the synthetic image set of the small part, and the workflow to train a classification model using the dataset that combines both generated image sets.

\subsubsection{Synthetic Image Set Generation}
Figure \ref{fig:AD1} depicts the workflow of activities required to generate a synthetic image set. The 3DModelCreator uses a 3D scanner to create a 3D model of a small part. Next, the DataGenerator sets the 3D model transformations and creates the synthetic scene in the 3D modeling software. Finally, the software is used to generate a synthetic image set.

\begin{figure}[H]
\centering
  \includegraphics[width=0.85\textwidth]{AD1}
\caption{Activity Diagram depicting the workflow required for synthetic image set generation.}
\label{fig:AD1}
\end{figure}

\subsubsection{Real Image Set Generation}
The workflow of activities required to generate a real image set is depicted in figure \ref{fig:AD2}. The DataGenerator selects a small part. The small part is transformed and the camera is used to take a picture. The process is repeated until the desired number of real images is reached.

\begin{figure}[H]
\centering
  \includegraphics[width=0.75\textwidth]{AD2}
\caption{Activity Diagram depicting the workflow required for real image set generation.}
\label{fig:AD2}
\end{figure}

\subsubsection{Image Classification}
Figure \ref{fig:AD3} describes the workflow of activities required to create the image classifier. The MachineLearningEngineer splits the data into training, validation and testing sets. The MachineLearningEngineer then creates a CNN model. The training and validation sets are used to train the CNN model. Next, the model accuracy is evaluated using the testing set. If the accuracy of the trained CNN model is not sufficient, the hyperparameters are fine tuned and the CNN model re-trained.

\begin{figure}[h]
\centering
  \includegraphics[width=0.85\textwidth]{AD3}
\caption{Activity Diagram describing the workflow required to train a CNN model and create an image classifier.}
\label{fig:AD3}
\end{figure}
