\chapter{Related Work}\label{ch:related_work}

We provide in this chapter the literature review about topics which touch different aspects of our work. Section \ref{sec:cnn_based_image_classification} discusses CNN based image classification, inspired by the overview of deep learning in neural networks presented by Schmidhuber \cite{schmidhuber2015deep}. Section \ref{sec:industrial_usecases} provides an overview of different industrial use cases of image classification. Finally, in section \ref{sec:synthetic_usage} we examine the use of synthetic images for different computer vision problems.


\section{CNN Based Image Classification}\label{sec:cnn_based_image_classification}

The \textit{Neocognitron}, introduced by Fukushima \cite{fukushima1980neocognitron}, is the first neural network that resembles the modern convolutional neural network. It was based on the neurophysical insights of Hubel et al. \cite{hubel1959receptive} \cite{hubel1962receptive}, where simple cells and complex cells were observed to respond differently to stimulation in the catâ€™s visual cortex. Subsequently, Fukushima introduced \textit{S-cells} (corresponding to simple cells), where a weighted filter is shifted accross a 2 dimensional array to produce activations of higher order, in order to detect a particular pattern (eg. edge, shape, etc..). Fukushima also introduced \textit{C-cells} (corresponding to complex cells), which are in turn wired to the output activations of a set of S-cells. C-cells are activated if any of the preceding S-cells are activated, hence they were used as subsampling layers. The Neocognitron is a multilayered network of S-cells and C-cells. The structure of a Neocognitron is similar to what we currently know as a convolutional neural network. Fukushima applied the Neocognitron to hand-written character recognition.

In 1989, LeCun et al. used the backpropagation algorithm to train a 5-layer network (dubbed \textit{LeNet-5}) \cite{lecun1989generalization} \cite{lecun1998gradient}. Similar to the Neocognitron, there are two types of layers. Convolution layers use a weighted sliding window to accross the 2-dimensional input to calculate the layer activations. On the other hand, subsampling layers calculate the average of neighbouring input activations. LeCun et al. trained LeNet-5 on 16x16 images of handwritten digits \cite{lecun1990handwritten}, and later applied this concept to develop an application for automatically reading zip codes \cite{lecun1989backpropagation}. This work also introduced the MNIST dataset, which is a database of handwritten digits \cite{lecun1998mnist}.

Inspired by the Neocognitron, Weng et al. introduced the Cresceptron \cite{weng1992cresceptron}, which adapts the same topology during training. To implement the subsampling layers, the Cresceptron uses Max-Pooling. Here, the 2-dimensional input to the subsampling layer is partitioned into smaller rectangular arrays. Each is replaced in the subsampling layer by the maximum activation value within the partition.

Traditionally, convolutional neural networks were implemented and computed on central processing units (CPU). To speed up the processing of CNNs Chellapilla et al. provided a graphical processing unit (GPU) based implementation of convolutional neural networks\cite{chellapilla2006high}. The GPU-based implementation was 4.11 times faster than the CPU-based implementation. GPUs have become more and more important for deep learning in subsequent years.

In 2011, Ciresan et al. \cite{ciresan2011flexible} described a GPU-implementation \cite{chellapilla2006high} of a CNN with max-pooling layers\cite{weng1992cresceptron}, trained using backpropagation \cite{lecun1989backpropagation}. Ciresan et al. used this to achieve superhuman visual pattern recognition performance in the IJCNN 2011 traffic sign recognition contest in San Jose, California \cite{stallkamp2011german} \cite{cirecsan2011committee} \cite{cirecsan2012multitraffic}. Ciresan et al. obtained a 0.56\% error rate, while the human performance on the test set was 1.19\%. Furthermore, Ciresan et al. achieved human-competitive performance (around 0.2\%) on MNIST handwriting benchmark \cite{cirecsan2012multimnist}.

The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images \cite{ILSVRC15}. The challenge has been conducted annually from 2010 to present. In 2012, Krizhevsky et al. \cite{krizhevsky2012imagenet} used a CNN (dubbed \textit{Alexnet}) to achieve the best results on the image classification benchmark. Alexnet achieved 16.4\% classification error, a significant improvement over the 2011 winning team, which achieved 25.8\%. Zeiler and Fergus \cite{zeiler2014visualizing} won the classification challenge in 2013. They presented a network which was based on Alexnet (dubbed \textit{ZF Net}) and were able to achieve 11.6\% classification error. Based on the work of Zeiler et al. \cite{zeiler2011adaptive}, Zeiler and Fergus developed a visualization technique called \textit{Deconvolutional Network}, which projects the feature activations back to the input pixel space. 2014 saw yet another CNN-based winning system. Szegedy et al. \cite{szegedy2015going} created a network called \textit{GoogLeNet} (also known as \textit{Inception}) that achieved 6.7\% classification error. In second place during the same year, Simonyan and Zisserman \cite{simonyan2014very} reached 7.3\% using the \textit{VGG} network.


\section{Industrial Use Cases for Image Classification}\label{sec:industrial_usecases}

Marino et al. \cite{marino2007real} \cite{de2009gpu} describe Visual Inspection System for Railway maintenance (VISyR), which detects the presence of hexagonal-headed bolts. VISyR uses two 3-layer neural networks (NN) running in parallel. The input features to the two networks are extracted using discrete wavelet transform (DWT). The first NN uses Daubechies wavelets, while the second one uses Haar wavelets. Both neural networks are trained with same examples and fined tuned using back propagation. The system signals the presence of a hexagonal-headed bolts if both networks detect its presence. 

Gibert et al. \cite{gibert2015robust} describes a method to detect the presence of a fastener in an image, determine whether the fastener is broken, and further subcategorize the fastener into one of five classes. The method uses Histogram of Gradients (HOG) to extract features, and a Support Vector Machine (SVM) for classification. In the same year, Gibert et al. \cite{gibert2015material} presented a way to classify materials in images of railway tracks. Gibert et al. use a 4-layered convolutional neural network to perform segmentation on the input images. The network classifies each pixel in the image into one of 10 materials. The CNN was trained to detect chipped or crumbling railway crossites. In 2017, Gibert et al. \cite{gibert2017deep} presented an approach to perform the tasks simultaneously: classify both the materials and the fasteners in an input image. Gibert et al. extended the material classification network \cite{gibert2015material} by extracting the features from the 3rd network layer. The features were used as input for an SVM classifier to detect the presence of a fastener and classify its type if found.

In 2015, Aytekin et al. \cite{aytekin2015railway} describe a real-time railway fastener detection system using a high-speed laser range finder camera, which is placed under a railway carriage designed for railway quality inspection. Aytekin et al. present multiple pixel-similarity-based approaches, namely principal component analysis (PCA), linear discriminant analysis (LDA), random-forest (RF), sparse representation (SR), and multitemplate matching (MTM). Moreover, the histogram-based approaches histogram matching (HM) and depth peeks (DP) approaches are also implemented. Aytekin et al. focused on false alarm rate (FAR) as an evaluation metric since they wanted to minimize false positives. Aytekin et al. concluded that a combination of PCA and DP produce the least false alarm rate.

Feng et al. \cite{feng2014automatic} describes a method to perform automatic fastener classification and defect detection. Their system which places 2 cameras under a train coach, which in turn takes pictures of the track and sends it to an onboard processor which performs fastener localization, classification and defect assessment by ranking the status of the fasteners. Feng et al. propose a novel classification technique based on Latent Dirichlet Allocation (LDA). Their system can model different types of fasteners using unlabeled data and is robust to illumination changes.


\section{Using Synthetic Images}\label{sec:synthetic_usage}

Synthetic images have been used for training in a variety of problems. Peng et al. \cite{peng2015learning} uses 3D CAD models to generate synthetic images. These images are then used to tests the invariance of convolutional neural networks to low level cues, namely, object texture, color, 3D pose and 3D shape, as well as background scene texture and color. A region-based convolutional neural network (RCNN) \cite{girshick2014rich} is used to test cue invariance in object detection. Peng et al. present the results of training a CNN on novel domains using synthetic images. They use part of the Office dataset \cite{saenko2010adapting}, which has the same 31 categories of common objects (cups, keyboards, etc.) in each domain. They compare the usage of 3D models with real texture versus 3D models with uniform gray texture. Peng et al. conclude that a CNN trained on synthetic images with real texture perform better than images with a gray texture.

Gerogakis et al. \cite{georgakis2017synthesizing} use a mixture of synthetic images and real images to train a network for object detection in indoor scenes. The synthetic images are created by superimposing images of the target objects on indoor backgrounds. Instead of rendering images from 3D models, cropped real images from the BigBird dataset \cite{singh2014bigbird} are used. Additionally, the background images have depth map information (RGB-D images), and come from the GMU-Kitchen Scenes dataset \cite{georgakis2016multiview} and the Washington RGB-D Scenes v2 datasets \cite{lai2014unsupervised}. Gerogakis et al. place the target object in the scene using a method they call \textit{selective positioning}. First, they use image segmentation to find counters and tables in the background image. Next, they use the depth map of the background image to scale the width and height of the target object. Lastly, they blend the object with the background image in order to mitigate the effects of changes in illumination and contrast. Gerogakis et al. conclude that training an object classifier using a mixture of 90\% synthetic images and 10\% real images can produce comparable results to a classifier trained on only real images.

Rajpura et al. \cite{rajpura2017object} used 3D models of refrigerators and and products that are placed in refrigerators from Archive3D\footnote{http://archive3d.net/} and ShapeNet \cite{chang2015shapenet} to perform object detection in refrigerator scenes. Synthetic images were generated by rendering 2D images of a 3D synthetic scene were the products are placed inside the refrigerators. Rajpura et al. trains a CNN using a fully real training set, a fully synthetic training set, and a synthetic training set containing 10\% real data. The CNN fully trained with only synthetic images  underperforms against one with real images but the mixed training set boosts the detection performance by 12\% which signifies the importance of transferable cues from synthetic to real.

Sarkar et al. \cite{sarkar2017trained} train a CNN image classifier on synthetic images rendered from 3D models of 5 different objects. The 3D models are created by scanning the real target objects using a 3D scanner. Synthetic images are created by rendering the 3D models on 3 different types of backgrounds: a plain white background, a random indoor background taken from indoor categories of PASCAL dataset \cite{everingham2010pascal}, and a chosen background similar to that of test images. Aditionally, images are rendered with two object texture settings: full texture and no texture. Sarkar et al. find that synthetic images of fully textured objects overlayed on a mixture of white and chosen backgrounds produce the best results.
