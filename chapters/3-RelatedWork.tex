\chapter{Related Work}\label{ch:related_work}

We provide in this chapter the literature review about topics which touch different aspects of our work. Section \ref{cnn} discusses CNN based image classification, inspired by the overview of deep learning in neural networks presented by Schmidhuber \cite{schmidhuber2015deep}. Section \ref{industrial} provides an overview of different industrial use cases of image classification. Finally, in section \ref{industrial} we examine the use of synthetic images for different computer vision problems.


\section{CNN Based Image Classification}\label{cnn}

The \textit{Neocognitron}, introduced by Fukushima \cite{fukushima1980neocognitron}, is the first neural network that resembles the modern convolutional neural network. It was based on the neurophysical insights of Hubel et al. \cite{hubel1959receptive} \cite{hubel1962receptive}, where simple cells and complex cells were observed to respond differently to stimulation in the catâ€™s visual cortex. Subsequently, Fukushima introduced \textit{S-cells} (corresponding to simple cells), where a weighted filter is shifted accross a 2 dimensional array to produce activations of higher order, in order to detect a particular pattern (eg. edge, shape, etc..). Fukushima also introduced \textit{C-cells} (corresponding to complex cells), which are in turn wired to the output activations of a set of S-cells. C-cells are activated if any of the preceding S-cells are activated, hence they were used as subsampling layers. The Neocognitron is a multilayered network of S-cells and C-cells. The structure of a Neocognitron is similar to what we currently know as a convolutional neural network. Fukushima applied the Neocognitron to hand-written character recognition.

In 1989, LeCun et al. used the backpropagation algorithm to train a 5-layer network (dubbed \textit{LeNet-5}) \cite{lecun1989generalization} \cite{lecun1998gradient}. Similar to the Neocognitron, there are two types of layers. Convolution layers use a weighted sliding window to accross the 2-dimensional input to calculate the layer activations. On the other hand, subsampling layers calculate the average of neighbouring input activations. LeCun et al. trained LeNet-5 on 16x16 images of handwritten digits \cite{lecun1990handwritten}, and later applied this concept to develop an application for automatically reading zip codes \cite{lecun1989backpropagation}. This work also introduced the MNIST dataset, which is a database of handwritten digits \cite{lecun1998mnist}.

Inspired by the Neocognitron, Weng et al. introduced the Cresceptron \cite{weng1992cresceptron}, which adapts the same topology during training. To implement the subsampling layers, the Cresceptron uses Max-Pooling. Here, the 2-dimensional input to the subsampling layer is partitioned into smaller rectangular arrays. Each is replaced in the subsampling layer by the maximum activation value within the partition.

Traditionally, convolutional neural networks were implemented and computed on central processing units (CPU). To speed up the processing of CNNs Chellapilla et al. provided a graphical processing unit (GPU) based implementation of convolutional neural networks\cite{chellapilla2006high}. The GPU-based implementation was 4.11 times faster than the CPU-based implementation. GPUs have become more and more important for deep learning in subsequent years.

In 2011, Ciresan et al. \cite{ciresan2011flexible} described a GPU-implementation \cite{chellapilla2006high} of a CNN with max-pooling layers\cite{weng1992cresceptron}, trained using backpropagation \cite{lecun1989backpropagation}. Ciresan et al. used this to achieve superhuman visual pattern recognition performance in the IJCNN 2011 traffic sign recognition contest in San Jose, California \cite{stallkamp2011german} \cite{cirecsan2011committee} \cite{cirecsan2012multitraffic}. Ciresan et al. obtained a 0.56\% error rate, while the human performance on the test set was 1.19\%. Furthermore, Ciresan et al. achieved human-competitive performance (around 0.2\%) on MNIST handwriting benchmark \cite{cirecsan2012multimnist}.

The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images \cite{ILSVRC15}. The challenge has been conducted annually from 2010 to present. In 2012, Krizhevsky et al. \cite{krizhevsky2012imagenet} used a CNN (dubbed \textit{Alexnet}) to achieve the best results on the image classification benchmark. Alexnet achieved 16.4\% classification error, a significant improvement over the 2011 winning team, which achieved 25.8\%. Zeiler and Fergus \cite{zeiler2014visualizing} won the classification challenge in 2013. They presented a network which was based on Alexnet (dubbed \textit{ZF Net}) and were able to achieve 11.6\% classification error. Based on the work of Zeiler et al. \cite{zeiler2011adaptive}, Zeiler and Fergus developed a visualization technique called \textit{Deconvolutional Network}, which projects the feature activations back to the input pixel space. 2014 saw yet another CNN-based winning system. Szegedy et al. \cite{szegedy2015going} created a network called \textit{GoogLeNet} (also known as \textit{Inception}) that achieved 6.7\% classification error. In second place during the same year, Simonyan and Zisserman \cite{simonyan2014very} reached 7.3\% using the \textit{VGG} network.


\section{Industrial Use Cases for Image Classification}\label{industrial}

Marino et al. \cite{marino2007real} \cite{de2009gpu} describe Visual Inspection System for Railway maintenance (VISyR), which detects the presence of hexagonal-headed bolts. VISyR uses two 3-layer neural networks (NN) running in parallel. The input features to the two networks are extracted using discrete wavelet transform (DWT). The first NN uses Daubechies wavelets, while the second one uses Haar wavelets. Both neural networks are trained with same examples and fined tuned using back propagation. The system signals the presence of a hexagonal-headed bolts if both networks detect its presence. 

Gibert et al. \cite{gibert2015robust} describes a method to detect the presence of a fastener in an image, determine whether the fastener is broken, and further subcategorize the fastener into one of five classes. The method uses Histogram of Gradients (HOG) to extract features, and a Support Vector Machine (SVM) for classification. In the same year, Gibert et al. \cite{gibert2015material} presented a way to classify materials in images of railway tracks. Gibert et al. use a 4-layered convolutional neural network to perform segmentation on the input images. The network classifies each pixel in the image into one of 10 materials. The CNN was trained to detect chipped or crumbling railway crossites. In 2017, Gibert et al. \cite{gibert2017deep} presented an approach to perform the tasks simultaneously: classify both the materials and the fasteners in an input image. Gibert et al. extended the material classification network \cite{gibert2015material} by extracting the features from the 3rd network layer. The features were used as input for an SVM classifier to detect the presence of a fastener and classify its type if found.

In 2015, Aytekin et al. \cite{aytekin2015railway} describe a real-time railway fastener detection system using a high-speed laser range finder camera, which is placed under a railway carriage designed for railway quality inspection. Aytekin et al. present multiple pixel-similarity-based approaches, namely principal component analysis (PCA), linear discriminant analysis (LDA), random-forest (RF), sparse representation (SR), and multitemplate matching (MTM). Moreover, the histogram-based approaches histogram matching (HM) and depth peeks (DP) approaches are also implemented. Aytekin et al. focused on false alarm rate (FAR) as an evaluation metric since they wanted to minimize false positives. Aytekin et al. concluded that a combination of PCA and DP produce the least false alarm rate.

Feng et al. \cite{feng2014automatic} describes a method to perform automatic fastener classification and defect detection. Their system which places 2 cameras under a train coach, which in turn takes pictures of the track and sends it to an onboard processor which performs fastener localization, classification and defect assessment by ranking the status of the fasteners. Feng et al. propose a novel classification technique based on Latent Dirichlet Allocation (LDA). Their system can model different types of fasteners using unlabeled data and is robust to illumination changes.


\section{Using Synthetic Images}\label{synthetic}

Synthetic images have been used for training in a variety of problems. Peng et al. \cite{peng2015learning} uses 3D CAD models to generate synthetic images. These images are then used to tests the invariance of convolutional neural networks to low level cues, namely, object texture, color, 3D pose and 3D shape, as well as background scene texture and color. Peng et al. uses RCNN \cite{girshick2014rich} to test cue invariance in object detection. 

The work done in \cite{rajpura2017object} train a CNN object detector to recognize objects inside a fridge. The work renders non-photorealistic synthetic scenes of 55 distinct products inside a fridge. The work evaluates the use of a fully synthetic training set versus a training set which includes 10\% real data.

In the same manner as \cite{rajpura2017object}, the work done in \cite{georgakis2017synthesizing} synthesizes training data for object detection in indoor scenes. The work creates synthetic data by superimposing 3D models on real scenes. The work compares purely synthetic training sets as well as different ratio of real to synthetic training images.

The work in \cite{sarkar2017trained} trains a CNN image classifier on synthetic images rendered from 3D models of 5 different objects.

The work in \cite{goyal2017dataset} uses synthetic images to perform semantic segmentation. The work renders synthetic images using 3D models. In addition, pixel-wise annotations of the synthetic images are generated. A CNN is trained on the annotated synthetic images and the intersection over union (IoU) is evaluated.